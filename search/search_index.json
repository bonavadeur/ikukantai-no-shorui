{"config":{"lang":["en","vi","ja"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Ikukantai - \u884c\u304f\u8266\u968a - The Iku Fleet","text":"<p><code>ikukantai</code> is a Serverless Computing platform designed for Distributed Computational Infrastructure Systems. It is developed based on Knative Serving, a leading open-source framework for Serverless Computing. <code>ikukantai</code> aims to be a platform that supports research and integration of algorithms related to Load Balancing, Container Scheduling, and Priority Queuing in the renowned Kubernetes container virtualization environment. <code>ikukantai</code> offers an easy system programming experience for scientists with limited resources to deploy practical testbeds, facilitating scientific research.</p> <p><code>ikukantai</code> was accepted at the IEEE CloudNet International Scientific Conference in November 2024 in Brazil, through the paper: FaaSt: A Latency-aware Serverless Scheme for Edge\u2013Cloud Environments. <code>FaaSt</code> served as the proof-of-concept (PoC) version of <code>ikukantai</code> for the scientific publication, and <code>ikukantai</code> is the official name for this project.</p> <p>The permission of operational, exploitation and deployment for the <code>ikukantai</code> Fleet, encompassing all rights related to its distribution and usage in publications are exclusively held and managed by Bonavadeur Tokyo.</p> <p></p>"},{"location":"architecture/","title":"Inside the Fleet: the Knative Serving underneath","text":""},{"location":"how-to-start/","title":"How to become sailor of Ikukantai Fleet?","text":""},{"location":"installation/","title":"Installation","text":""},{"location":"installation/#1-system-requirements","title":"1. System requirements","text":"<ul> <li>Some nodes are Physical Machine or Virtual Machine, least 4 CPU and 16GB RAM for master-node and  3 CPU 6GB RAM for each worker-nodes  </li> <li>Ubuntu-Server or Ubuntu Desktop version 20.04  </li> <li>Kubernetes version 1.26.3  </li> <li>Calico installed on Kubernetes cluster  </li> <li>MetalLB installed on Kubernetes cluster (for laboratory experiments, we deploy system on a bare-metal cluster)  </li> <li>Helm is installed</li> </ul>"},{"location":"installation/#2-install-support-mechanisms","title":"2. Install support mechanisms","text":""},{"location":"installation/#21-monlat-the-network-latency-monitoring-system-for-kubernetes","title":"2.1. Monlat - the network latency monitoring system for Kubernetes","text":"<p>We develop a network latency monitoring system named <code>monlat</code>, for more detail and installation please visit monlat. First, let's install Prometheus Stack on Kubernetes Cluster, then install <code>monlat</code> later. The network latency metrics will be collected by Prometheus.</p>"},{"location":"installation/#install-prometheus-stack","title":"Install Prometheus Stack","text":"<p>We follow Prometheus Stacks installation guide from Knative's Docs</p> <pre><code>$ helm repo add prometheus-community https://prometheus-community.github.io/helm-charts\n$ helm repo update\n$ helm install prometheus prometheus-community/kube-prometheus-stack -n default -f manifest/prometheus/values.yaml\n\n$ kubectl apply -f https://raw.githubusercontent.com/knative-extensions/monitoring/main/grafana/dashboards.yaml\n\n$ kubectl create namespace metrics\n$ kubectl apply -f https://raw.githubusercontent.com/knative/docs/main/docs/serving/observability/metrics/collector.yaml\n</code></pre> <p>Note: The OpenTelemetry Collector logging exporter is deprecated and removed from the upstream Collector repository in September 2024, so that you have to change all <code>logging</code> fields to <code>debug</code> using the following commands:</p> <pre><code>chmod +x ./hack/fix-otel-collector-cfg.sh\n./hack/fix-otel-collector-cfg.sh\n</code></pre>"},{"location":"installation/#install-monlat","title":"Install monlat","text":"<p>Follow monlat installation guide to install <code>monlat</code> correctly. <code>monlat</code> is released under Apache License.</p>"},{"location":"installation/#22-seika-kubernetes-custom-resource-maintains-quantity-of-pod-in-each-node","title":"2.2. Seika - Kubernetes Custom Resource maintains quantity of Pod in each Node","text":"<p>To control the ability of create and delete Function in each Node exactly, we develop a Kubernetes Custom Resource named Seika. Seika operate like a bunch of Deployments that each Deployment control number of Pod in only one Node. By using Seika, we can create more Functions in Node that have more traffic and delete less Functions in Node that have less traffic. To install, please visit Seika. The Seika's document includes guides for installation, usage, development. <code>Seika</code> is released under Apache License.</p>"},{"location":"installation/#3-install-knative-serving-with-kourier-is-networking-option-and-our-extra-controller","title":"3. Install Knative Serving with Kourier is networking option and Our Extra-Controller","text":"<p>In this step we install Knative Serving's components (CRD, Knative's Pods) by applying .yaml files. Notes that the applied manifests is modified by ours, we do not use the original images and configurations. Our images are developed base on Knative-Serving version 1.12.1 and Kourier version 1.12.1</p> <pre><code># Install CRD\nkubectl apply -f manifest/1-serving-crd.yaml\n# Install Knative's Pod\nkubectl apply -f manifest/2-serving-core.yaml\n# Extra configmap and RBAC\nkubectl apply -f manifest/miporin/configmap.yaml\nkubectl apply -f manifest/miporin/rbac.yaml\n# Install Networking Plugin\nkubectl apply -f manifest/3-kourier.yaml\n# Run domain config job\nkubectl apply -f manifest/4-serving-default-domain.yaml\n</code></pre> <p>Wait until job/default-domain is success</p> <pre><code># check if default-domain job is success\nkubectl -n knative-serving get job | grep default-domain\nNAME             COMPLETIONS   DURATION   AGE\ndefault-domain   1/1           13s        71s\n# delete config job\nkubectl delete -f manifest/4-serving-default-domain.yaml\n</code></pre> <p>Install extra-controller <code>miporin</code></p> <pre><code>kubectl apply -f manifest/miporin/miporin.yaml\n</code></pre> <p><code>miporin</code> is the extra-controller working alongside and is independently of Knative's controller. For more information about <code>miporin</code>, please visit bonavadeur/miporin. Miporin is released under Apache License.</p> <p>Install correct images by version</p> <pre><code># Replace Knative's images by Ikukantai's images\nchmod +x -R hack/*\n./hack/replace-image.sh\n</code></pre>"},{"location":"installation/#4-making-some-changes","title":"4. Making some changes","text":""},{"location":"installation/#41-kourier-gateway","title":"4.1. Kourier Gateway","text":"<pre><code># use local 3scale-kourier-gateway pod for every request\nkubectl -n kourier-system patch service kourier --patch '{\"spec\":{\"internalTrafficPolicy\":\"Local\",\"externalTrafficPolicy\":\"Local\"}}'\nkubectl -n kourier-system patch service kourier-internal --patch '{\"spec\":{\"internalTrafficPolicy\":\"Local\"}}'\n</code></pre>"},{"location":"installation/#5-check-your-setup","title":"5. Check your setup","text":"<p>You must see 3scale-kourier-gateway and activator present in all nodes, each node has one activator and one 3scale-kourier-gateway</p> <pre><code>$ kubectl -n knative-serving get pod -o wide | grep activator\nactivator-5cd6cb5f45-5nnnb                1/1     Running     0                156m   10.233.75.29     node2   &lt;none&gt;           &lt;none&gt;\nactivator-5cd6cb5f45-fkp2r                1/1     Running     0                156m   10.233.102.181   node1   &lt;none&gt;           &lt;none&gt;\nactivator-5cd6cb5f45-j6bqq                1/1     Running     0                156m   10.233.71.47     node3   &lt;none&gt;           &lt;none&gt;\n\n$ kubectl -n kourier-system get pod -o wide\nNAME                                     READY   STATUS    RESTARTS         AGE    IP               NODE    NOMINATED NODE   READINESS GATES\n3scale-kourier-gateway-864554589-5dgxl   1/1     Running   11 (5h26m ago)   2d5h   10.233.75.28     node2   &lt;none&gt;           &lt;none&gt;\n3scale-kourier-gateway-864554589-btfqf   1/1     Running   12 (5h21m ago)   2d5h   10.233.71.29     node3   &lt;none&gt;           &lt;none&gt;\n3scale-kourier-gateway-864554589-p7q56   1/1     Running   13 (5h29m ago)   2d5h   10.233.102.176   node1   &lt;none&gt;           &lt;none&gt;\n\n$ kubectl -n knative-serving get pod | grep miporin\nmiporin-597dcddbc-qvlc6                   1/1     Running     0                143m\n</code></pre>"},{"location":"installation/#6-try-it-out","title":"6. Try it out","text":"<p>Each time you deploy a ksvc (in API service.serving.knative.dev), <code>ikukantai</code> will create two custom resources automatically: one Seika and one ServiceMonitor.</p> <ul> <li>Seika (in API seika.batch.bonavadeur.io) is used for ability of controlling Function creation and deletion precisely in each Node</li> <li>ServiceMonitor (in API servicemonitor.monitoring.coreos.com) is used for scraping metrics of each Function to Prometheus</li> </ul> <p>First, apply a simple web application named hello. The annotation autoscaling.knative.dev/window: \"12s\" means that if there is not traffic come to system in 12s, the Function will be scaled down. Immediately after you apply hello Function, the first Pod created is not under your control hello. You need to wait until this pod deleted after 12s, the system is now under your control.</p> <pre><code># install a demo app\n$ kubectl apply -f manifest/demo/hello.yaml\nWarning: Kubernetes default value is insecure, Knative may default this to secure in a future release: spec.template.spec.containers[0].securityContext.allowPrivilegeEscalation, spec.template.spec.containers[0].securityContext.capabilities, spec.template.spec.containers[0].securityContext.runAsNonRoot, spec.template.spec.containers[0].securityContext.seccompProfile\nservice.serving.knative.dev/hello created\n</code></pre> <p>Get all relevant resources:</p> <pre><code>$ kubectl get ksvc,pod,seika,servicemonitor | grep hello\nservice.serving.knative.dev/hello   http://hello.default.192.168.133.2.sslip.io   hello-00001    hello-00001   True    \npod/hello-00001-deployment-7df54dc57f-rx7xr                  2/2     Running   0               11s\nseika.batch.bonavadeur.io/hello   [\"node1\",\"node2\",\"node3\"]   0-0-0/0-0-0\nservicemonitor.monitoring.coreos.com/hello\n</code></pre> <p>Wait until the first Pod is deleted (after the period set by annotation autoscaling.knative.dev/window: \"12s\")</p> <pre><code>$ kubectl get ksvc,pod,seika,servicemonitor | grep hello\nservice.serving.knative.dev/hello   http://hello.default.192.168.133.2.sslip.io   hello-00001    hello-00001   True\nseika.batch.bonavadeur.io/hello   [\"node1\",\"node2\",\"node3\"]   0-0-0/0-0-0\nservicemonitor.monitoring.coreos.com/hello\n</code></pre> <p>Use <code>netem</code> setup latency between nodes. In this experiment, I setup latency from node1, node2 to node3 is 50ms. So, when make request from node3, a new Pod is prefer scheduled on node3 instead of the remain nodes.</p> <pre><code># make request from node3\nroot@node3:~$ curl hello.default.svc.cluster.local\nKonnichiwa from hello-node3-xgvq5 in node3\n\n# list all resources\nroot@node1:~$ kubectl get ksvc,pod,seika,servicemonitor | grep hello\nservice.serving.knative.dev/hello   http://hello.default.192.168.133.2.sslip.io   hello-00001   hello-00001   True\npod/hello-node3-xgvq5                                        2/2     Running   0               40s\nseika.batch.bonavadeur.io/hello   [\"node1\",\"node2\",\"node3\"]   0-0-1/0-0-1\nservicemonitor.monitoring.coreos.com/hello\n</code></pre> <p>The Scheduling Algorithm is implemented in miporin, package <code>github.com/bonavadeur/miporin/pkg/yukari</code>. To enable Scheduling Feature of <code>ikukantai</code> Fleet, set config <code>ikukantai-miporin-enable-yukari: \"true\"</code> in <code>configmap/config-ikukantai</code>, namespace <code>default</code></p>"},{"location":"features/","title":"Overview","text":"<p><code>ikukantai</code> is closed-source, but you can exploit all extra power by using tanks deployed on the flight deck of the Fleet. We have a plan for developing 4 extra-components that make algorithm implementation easier in the near future.</p> <p>Miporin - tank commander, the extra-controller working alongside and is independently of Knative's controller. <code>miporin</code> also act as Scheduler of the Fleet. <code>miporin</code> is written in Go and release under Apache-2.0 License.</p> <p>Katyusha - Load Balancing Algorithm Implementation Module on the Fleet, written in Go, released under Apache-2.0 License.</p> <p>Nonna - Queue Modifier Module on the Fleet, written in Go, released under Apache-2.0 License.</p> <p>Panzer vor!</p>"},{"location":"features/load-balancer/","title":"LoadBalancer: Katyusha - \u041a\u0430\u0442\u044e\u0448\u0430 - \u30ab\u30c1\u30e5\u30fc\u30b7\u30e3","text":"<p>(Snowdrift Katyusha - Tuy\u1ebft Phong Katyusha - \u5730\u5439\u96ea\u306e\u30ab\u30c1\u30e5\u30fc\u30b7\u30e3)</p> <p> </p> <p> </p> <p><code>Katyusha-sama</code> is the Load Balancing Module of the ikukantai Fleet.</p> <p></p>"},{"location":"features/queue-modifier/","title":"Queue Modifier: Nonna - \u041d\u043e\u043d\u043d\u0430 - \u30ce\u30f3\u30ca","text":"<p>(Blizzard Nonna - B\u00e3o Tuy\u1ebft Nonna - \u30d6\u30ea\u30b6\u30fc\u30c9\u306e\u30ce\u30f3\u30ca)</p> <p> </p> <p> </p> <p><code>Nonna</code> is the Queue Modifier Module of the ikukantai Fleet</p> <p></p>"},{"location":"features/scheduler/","title":"Scheduler and ExtraController: Miporin - \u307f\u307d\u308a\u3093","text":"<p><code>miporin</code>-chan is the extra-controller of <code>ikukantai</code> Fleet, working alongside and independently of Knative's controller.</p> <p></p>"},{"location":"katyusha/","title":"LoadBalancer: Katyusha - \u041a\u0430\u0442\u044e\u0448\u0430 - \u30ab\u30c1\u30e5\u30fc\u30b7\u30e3","text":"<p>(Snowdrift Katyusha - Tuy\u1ebft Phong Katyusha - \u5730\u5439\u96ea\u306e\u30ab\u30c1\u30e5\u30fc\u30b7\u30e3)</p> <p> </p> <p> </p> <p><code>Katyusha-sama</code> is the Load Balancing Module of the ikukantai Fleet.</p> <p></p> <p><code>katyusha</code> supports deploying Load-Balancing Algorithm in <code>ikukantai</code> Fleet without deeping into Knative source code.</p> <p>The name <code>katyusha</code> is inspired by the character Katyusha in the anime Girls und Panzer. <code>katyusha</code> and nonna form a complete Load Balancing system for the <code>ikukantai</code> Fleet. This Load Balancing system uses piggybacking mechanism to update load status as fast as possible, much like how Nonna always carries Katyusha on her back in anime Girls und Panzer.</p> <p><code>katyusha</code> is also inspired by the Soviet BM-13 Katyusha rocket launcher in the Great Patriotic War (just like Katyusha in Girls und Panzer). Our Load Balancing system operates as efficiently and powerfully as Katyusha!</p> <p>According to the anime Girls und Panzer, the character Katyusha really hates speaking Russian and always demands team members speak Japanese. Therefore, some prominent Golang structs and packages in this source code are named in Japanese! \u65e5\u672c\u8a9e\u3092\u8a71\u3057\u3066\u304f\u3060\u3055\u3044\u3088\uff01\uff01\uff01</p>"},{"location":"katyusha/architecture/","title":"Architecture of Katyusha","text":""},{"location":"katyusha/development/","title":"Development","text":"<p>First, apply a demo application for <code>nonna</code> development:</p> <pre><code>$ kubectl apply -f config/hello.yaml\n</code></pre> <p>Take a look in build.sh. There are two options for building <code>katyusha</code> image: + ful: build <code>katyusha</code> image from source, fastly but large size of image + push: like ful options, but slower and smaller size of image because the compression level is increased and then image will be pushed to the registry</p> <pre><code>$ ./build.sh ful # faster but larger image size\n$ ./build.sh push # slower but smaller image size\n</code></pre>"},{"location":"katyusha/installation/","title":"Installation","text":""},{"location":"katyusha/installation/#1-requirement","title":"1. Requirement","text":"<ul> <li>ikukantai Fleet is deployed, version &gt;= 2.2</li> <li>Go is installed, version &gt;= 1.22.4</li> <li>Docker is installed. <code>docker</code> command can be invoked without sudo</li> <li>upx is installed, version &gt;= 4.2.4</li> </ul>"},{"location":"katyusha/installation/#2-config-katyusha-as-image-of-activator","title":"2. Config <code>katyusha</code> as image of Activator","text":"<p>Let's assume that the nodes in your cluster are named node1, node2, node3. See <code>spec.template.spec.affinity</code> to fill this information into the activator-deployment.yaml file correctly.</p> <p>Change Activator from deployed as DaemonSet to Deployment</p> <pre><code>$ kubectl -n knative-serving delete daemonset activator\n$ kubectl apply -f config/activator-deployment.yaml\n</code></pre> <p>New raising Activator will deployed with image <code>katyusha:dev</code></p> <pre><code>$ kubectl -n knative-serving get deploy activator -o yaml | grep image:\n        image: docker.io/bonavadeur/katyusha:dev\n</code></pre>"},{"location":"katyusha/installation/#3-add-configs-to-configmap-for-katyusha","title":"3. Add configs to Configmap for Katyusha","text":"<p><code>katyusha</code> uses these configs in ConfigMap config-ikukantai in namespace knative-serving:</p> Config Description Value Example ikukantai-enable-katyusha enable/disable <code>katyusha</code> bool \"true\", \"false\" katyusha-threads number of processing threads in <code>katyusha</code> integer \"10\" katyusha-enable-junbanmachi enable/disable Layer-1 Queuing feature bool \"true\", \"false\" katyusha-enable-fukabunsan enable/disable Load Balancing feature bool \"true\", \"false\" katyusha-enable-outoushuugou enable/disable Response Pool feature bool \"true\", \"false\" katyusha-junbanmachi-concurrent-request number of concurrent request in Layer-1 Queue integer \"10\" <p>Example:</p> <pre><code>apiVersion: v1\nkind: ConfigMap\nmetadata:\n  name: config-ikukantai\n  namespace: knative-serving\n...\ndata:\n  ...\n  ikukantai-enable-katyusha: 'true'\n  katyusha-enable-fukabunsan: 'true'\n  katyusha-enable-junbanmachi: 'true'\n  katyusha-enable-outoushuugou: 'true'\n  katyusha-junbanmachi-concurrent-request: '10'\n  katyusha-threads: '10'\n  ...\n</code></pre>"},{"location":"katyusha/learn/","title":"Say \"konnichiwa\" to Katyusha","text":""},{"location":"katyusha/references/","title":"References","text":""},{"location":"miporin/","title":"Scheduler and ExtraController: Miporin - \u307f\u307d\u308a\u3093","text":"<p><code>miporin</code>-chan is the extra-controller of <code>ikukantai</code> Fleet, working alongside and independently of Knative's controller.</p> <p></p> <p>To achieve the goals posed by the <code>ikukantai</code> Fleet, in addition to modifying Knative's source code, we needed a component acts as a controller that exploits the refined code inside Knative. In theory, we can develop additional logic in Knative's controller component. However, that will be more difficult than developing an extra external component for PoC purposes in the Laboratory (yaa, we work in the Laboratory, not Industry).</p> <p>The name <code>miporin</code> is inspired by the character <code>Nishizumi Miho</code> from the anime <code>Girls und Panzer</code>. Miho is the tank commander, implying <code>miporin</code>'s leadership role in the <code>ikukantai</code> fleet (remember that Ooarai High School is located in an aircraft carrier, and, <code>ikukantai</code> is implied to be that ship). <code>miporin</code> is nickname given to Miho by her friends.</p>"},{"location":"miporin/architecture/","title":"Architecture of Miporin","text":""},{"location":"miporin/development/","title":"Development","text":"<p>Firstly, modify image used by deployment/miporin in namespace knative-serving by image named <code>docker.io/bonavadeur/miporin:dev</code>. A new Pod miporin will be raised up due to the previous changes, and this Pod will be failed. Next, build your own image for development environment:</p> <pre><code>$ kubectl -n knative-serving patch deploy miporin --patch '{\"spec\":{\"template\":{\"spec\":{\"containers\":[{\"name\":\"miporin\",\"image\":\"docker.io/bonavadeur/miporin:dev\"}]}}}}'\n$ chmod +x ./build.sh\n$ ./build.sh ful\n</code></pre> <p>Change Endpoint IP address to IP of your machine for running <code>miporin</code> by binary:</p> <pre><code># file ./config/localdev.yaml\napiVersion: discovery.k8s.io/v1\nkind: EndpointSlice\nmetadata:\n  name: miporin-localdev\n  namespace: knative-serving\n...\nendpoints:\n  - addresses:\n    - \"192.168.122.100\" # change this to be your IP, example: 192.168.189.22\n</code></pre> <p>Some util commands</p> <pre><code># grant execute permission to build.sh file\nchmod +x ./build.sh\n# run code directly by binary\n./build.sh local\n# run miporin as a container\n./build.sh ful\n# push miporin image to docker registry\n./build.sh push &lt;tag&gt;\n</code></pre>"},{"location":"miporin/installation/","title":"Installation","text":""},{"location":"miporin/installation/#1-requirement","title":"1. Requirement","text":"<ul> <li>ikukantai Fleet is deployed, version &gt;= 2.0</li> <li>ko build is installed, version 0.16.0</li> <li>Go is installed, version &gt;= 1.22.4</li> <li>Docker is installed. <code>docker</code> command can be invoked without sudo</li> </ul>"},{"location":"miporin/installation/#2-installation","title":"2. Installation","text":"<p><code>miporin</code> is deployed in namespace knative-serving</p> <pre><code>kubectl apply -f config/miporin.yaml\n</code></pre>"},{"location":"miporin/learn/","title":"Say \"konnichiwa\" to Miporin","text":""},{"location":"miporin/references/","title":"References","text":""},{"location":"nonna/","title":"Queue Modifier: Nonna - \u041d\u043e\u043d\u043d\u0430 - \u30ce\u30f3\u30ca","text":"<p>(Blizzard Nonna - B\u00e3o Tuy\u1ebft Nonna - \u30d6\u30ea\u30b6\u30fc\u30c9\u306e\u30ce\u30f3\u30ca)</p> <p> </p> <p> </p> <p><code>Nonna</code> is the Queue Modifier Module of the ikukantai Fleet</p> <p></p> <p><code>nonna</code> supports deploying Queue-Modifying Algorithm in <code>ikukantai</code> Fleet without deeping into Knative source code.</p> <p><code>nonna</code> is actually Knative Queue-Proxy underneath. In the Vanilla Knative Serving, The queueing model implemented in Queue-Proxy is FIFO Queue. Although FIFO is a simple queueing model, it is not the most optimal queueing model in some complicated scenarios. By using <code>nonna</code>, you can implement your own priority queue that adopts parameters such as: HTTP method, URI path, HTTP Header, Source IP address and Domain name.</p> <p><code>nonna</code> also supports piggybacking based Load Balancer in the <code>ikukantai</code> Fleet along with katyusha by modifying HTTP header of responsed packets.</p> <p>The name <code>nonna</code> is inspired by the character Nonna in the anime Girls und Panzer. <code>nonna</code> and katyusha form a complete Load Balancing system for the <code>ikukantai</code> Fleet. This Load Balancing system uses piggybacking mechanism to update load status as fast as possible, much like how Nonna always carries Katyusha on her back in anime Girls und Panzer.</p>"},{"location":"nonna/architecture/","title":"Architecture of Nonna","text":""},{"location":"nonna/development/","title":"Development","text":"<p>First, apply a demo application for <code>nonna</code> development:</p> <pre><code>$ kubectl apply -f config/hello.yaml\n</code></pre> <p>Make sure <code>nonna</code> is injected into the Pod:</p> <pre><code>$ kubectl get pod | grep hello\nhello-00001-deployment-598589db69-bhwhv                  2/2     Running   0              26m\n\n$ kubectl get pod hello-00001-deployment-598589db69-bhwhv -o yaml | grep image:\n    image: index.docker.io/bonavadeur/shuka@sha256:92b17a46559202b3584a3e9e1373914ed0e66bc55ba6d3a1353312dae25de79b\n    image: docker.io/bonavadeur/nonna:dev\n    image: docker.io/bonavadeur/nonna:dev\n    image: sha256:79054189220aa9f84fac527b4069f4ed17b6de4e6713d5d58ccfe06a17ea0dd6\n</code></pre> <p>Take a look in build.sh. There are two options for building <code>nonna</code> image: + ful: build <code>nonna</code> image from source, fastly but large size of image + push: like ful options, but slower and smaller size of image because the compression level is increased and then image will be pushed to the registry</p> <pre><code>$ ./build.sh ful # faster but larger image size\n$ ./build.sh push # slower but smaller image size\n</code></pre>"},{"location":"nonna/installation/","title":"Installation","text":""},{"location":"nonna/installation/#1-requirement","title":"1. Requirement","text":"<ul> <li>ikukantai Fleet is deployed, version &gt;= 2.1</li> <li>Go is installed, version &gt;= 1.22.4</li> <li>Docker is installed. <code>docker</code> command can be invoked without sudo</li> <li>upx is installed, version &gt;= 4.2.4</li> </ul>"},{"location":"nonna/installation/#2-config-nonna-as-image-of-queue-proxy","title":"2. Config <code>nonna</code> as image of queue-proxy","text":"<p>Edit Custom Resource Image.caching.internal.knative.dev/v1alpha1 and Configmap config-deployment in namespace knative-serving to change image for queue-proxy to <code>nonna</code>, see replace-image.sh</p> <pre><code>$ chmod +x hack/replace-image.sh\n$ ./hack/replace-image.sh\n</code></pre>"},{"location":"nonna/installation/#3-add-configs-to-configmap-for-nonna","title":"3. Add configs to Configmap for Nonna","text":"<p><code>nonna</code> uses these configs in ConfigMap config-ikukantai in namespace knative-serving:</p> Config Description Value Example ikukantai-enable-nonna enable/disable <code>nonna</code> bool \"true\", \"false\" nonna-threads number of processing threads in <code>nonna</code> integer \"10\" <p>Example:</p> <pre><code>apiVersion: v1\nkind: ConfigMap\nmetadata:\n  name: config-ikukantai\n  namespace: knative-serving\n...\ndata:\n  ...\n  ikukantai-enable-nonna: 'true'\n  nonna-threads: '10'\n  ...\n</code></pre>"},{"location":"nonna/learn/","title":"Say \"konnichiwa\" to Nonna","text":""},{"location":"nonna/references/","title":"References","text":""},{"location":"vi/","title":"Ikukantai - \u884c\u304f\u8266\u968a - H\u1ea1m \u0110\u1ed9i Ik\u01b0","text":"<p><code>ikukantai</code> l\u00e0 n\u1ec1n t\u1ea3ng \u0110i\u1ec7n to\u00e1n Phi m\u00e1y ch\u1ee7, \u0111\u01b0\u1ee3c thi\u1ebft k\u1ebf cho H\u1ec7 th\u1ed1ng H\u1ea1 t\u1ea7ng t\u00ednh to\u00e1n ph\u00e2n t\u00e1n. <code>ikukantai</code> \u0111\u01b0\u1ee3c ph\u00e1t tri\u1ec3n t\u1eeb m\u00e3 ngu\u1ed3n m\u1edf h\u00e0ng \u0111\u1ea7u v\u1ec1 \u0110i\u1ec7n to\u00e1n Phi m\u00e1y ch\u1ee7 Knative Serving. <code>ikukantai</code> h\u01b0\u1edbng t\u1edbi l\u00e0 m\u1ed9t n\u1ec1n t\u1ea3ng h\u1ed7 tr\u1ee3 nghi\u00ean c\u1ee9u, t\u00edch h\u1ee3p c\u00e1c thu\u1eadt to\u00e1n li\u00ean quan \u0111\u1ebfn C\u00e2n b\u1eb1ng t\u1ea3i (Load Balancing), L\u1eadp l\u1ecbch container (scheduling) v\u00e0 h\u00e0ng \u0111\u1ee3i \u01b0u ti\u00ean trong m\u00f4i tr\u01b0\u1eddng \u1ea3o ho\u00e1 Container n\u1ed5i ti\u1ebfng Kubernetes. <code>ikukantai</code> mang l\u1ea1i tr\u1ea3i nghi\u1ec7m l\u1eadp tr\u00ecnh h\u1ec7 th\u1ed1ng d\u1ec5 d\u00e0ng cho c\u00e1c nh\u00e0 khoa h\u1ecdc c\u00f3 ngu\u1ed3n l\u1ef1c h\u1ea1n ch\u1ebf trong vi\u1ec7c tri\u1ec3n khai testbed th\u1ef1c t\u1ebf, ph\u1ee5c v\u1ee5 c\u00e1c nghi\u00ean c\u1ee9u khoa h\u1ecdc</p> <p><code>ikukantai</code> \u0111\u00e3 \u0111\u01b0\u1ee3c c\u00f4ng b\u1ed1 khoa h\u1ecdc t\u1ea1i H\u1ed9i th\u1ea3o Khoa h\u1ecdc Qu\u1ed1c t\u1ebf IEEE CloudNet v\u00e0o th\u00e1ng 11/2024 t\u1ea1i Brasil trong Paper: FaaSt: A Latency-aware Serverless Scheme for Edge\u2013Cloud Environments. <code>FaaSt</code> l\u00e0 phi\u00ean b\u1ea3n PoC c\u1ee7a <code>ikukantai</code> ph\u1ee5c v\u1ee5 cho c\u00f4ng b\u1ed1 khoa h\u1ecdc. <code>ikukantai</code> l\u00e0 t\u00ean ch\u00ednh th\u1ee9c cho d\u1ef1 \u00e1n n\u00e0y.</p> <p>B\u1ea3n quy\u1ec1n v\u1ec1 quy\u1ec1n v\u1eadn h\u00e0nh, khai th\u00e1c v\u00e0 tri\u1ec3n khai H\u1ea1m \u0110\u1ed9i <code>ikukantai</code>, bao g\u1ed3m t\u1ea5t c\u1ea3 c\u00e1c quy\u1ec1n ph\u00e2n ph\u1ed1i v\u00e0 s\u1eed d\u1ee5ng trong c\u00e1c c\u00f4ng b\u1ed1 khoa h\u1ecdc \u0111\u01b0\u1ee3c \u0111\u1ed9c quy\u1ec1n thu\u1ed9c v\u1ec1 Bonavadeur Tokyo</p> <p></p>"},{"location":"ja/","title":"Ikukantai - \u884c\u304f\u8266\u968a","text":"<p><code>ikukantai</code>\u306f\u3001\u5206\u6563\u8a08\u7b97\u30a4\u30f3\u30d5\u30e9\u30b9\u30c8\u30e9\u30af\u30c1\u30e3\u30b7\u30b9\u30c6\u30e0\u5411\u3051\u306b\u8a2d\u8a08\u3055\u308c\u305f\u30b5\u30fc\u30d0\u30fc\u30ec\u30b9\u30b3\u30f3\u30d4\u30e5\u30fc\u30c6\u30a3\u30f3\u30b0\u30d7\u30e9\u30c3\u30c8\u30d5\u30a9\u30fc\u30e0\u3067\u3059\u3002 \u30aa\u30fc\u30d7\u30f3\u30bd\u30fc\u30b9\u306e\u30b5\u30fc\u30d0\u30fc\u30ec\u30b9\u30b3\u30f3\u30d4\u30e5\u30fc\u30c6\u30a3\u30f3\u30b0\u30d5\u30ec\u30fc\u30e0\u30ef\u30fc\u30af\u3067\u3042\u308bKnative Serving\u3092\u30d9\u30fc\u30b9\u306b\u958b\u767a\u3055\u308c\u3066\u3044\u307e\u3059\u3002<code>ikukantai</code>\u306f\u3001Kubernetes\u3068\u3044\u3046\u8457\u540d\u306a\u30b3\u30f3\u30c6\u30ca\u4eee\u60f3\u5316\u74b0\u5883\u306b\u304a\u3044\u3066\u3001\u8ca0\u8377\u5206\u6563\uff08Load Balancing\uff09\u3001\u30b3\u30f3\u30c6\u30ca\u30b9\u30b1\u30b8\u30e5\u30fc\u30ea\u30f3\u30b0\uff08Scheduling\uff09\u3001\u304a\u3088\u3073\u512a\u5148\u30ad\u30e5\u30fc\u30a4\u30f3\u30b0\uff08Priority Queuing\uff09\u306b\u95a2\u9023\u3059\u308b\u30a2\u30eb\u30b4\u30ea\u30ba\u30e0\u306e\u7814\u7a76\u3068\u7d71\u5408\u3092\u652f\u63f4\u3059\u308b\u30d7\u30e9\u30c3\u30c8\u30d5\u30a9\u30fc\u30e0\u3092\u76ee\u6307\u3057\u3066\u3044\u307e\u3059\u3002<code>ikukantai</code>\u306f\u3001\u9650\u3089\u308c\u305f\u30ea\u30bd\u30fc\u30b9\u3057\u304b\u6301\u305f\u306a\u3044\u79d1\u5b66\u8005\u304c\u3001\u5b9f\u969b\u306e\u30c6\u30b9\u30c8\u30d9\u30c3\u30c9\u3092\u7c21\u5358\u306b\u5c55\u958b\u3057\u3001\u79d1\u5b66\u7814\u7a76\u3092\u9032\u3081\u308b\u305f\u3081\u306e\u30b7\u30b9\u30c6\u30e0\u30d7\u30ed\u30b0\u30e9\u30df\u30f3\u30b0\u3092\u5bb9\u6613\u306b\u3059\u308b\u74b0\u5883\u3092\u63d0\u4f9b\u3057\u307e\u3059\u3002</p> <p><code>ikukantai</code>\u306f\u30012024\u5e7411\u6708\u306b\u30d6\u30e9\u30b8\u30eb\u3067\u958b\u50ac\u3055\u308c\u305fIEEE CloudNet\u56fd\u969b\u5b66\u8853\u4f1a\u8b70\u3067\u3001\u8ad6\u6587\u300cFaaSt: A Latency-aware Serverless Scheme for Edge\u2013Cloud Environments\u300d\u3092\u901a\u3058\u3066\u767a\u8868\u3055\u308c\u307e\u3057\u305f\u3002 <code>FaaSt</code>\u306f\u3053\u306e\u30d7\u30ed\u30b8\u30a7\u30af\u30c8\u306e\u79d1\u5b66\u7684\u767a\u8868\u306e\u305f\u3081\u306ePoC\uff08\u6982\u5ff5\u5b9f\u8a3c\uff09\u7248\u3068\u3057\u3066\u6a5f\u80fd\u3057\u3066\u304a\u308a\u3001<code>ikukantai</code>\u306f\u3053\u306e\u30d7\u30ed\u30b8\u30a7\u30af\u30c8\u306e\u6b63\u5f0f\u540d\u79f0\u3067\u3059\u3002</p> <p><code>ikukantai</code>\u8266\u968a\u306e\u904b\u7528\u304a\u3088\u3073\u5c55\u958b\u6a29\u306f\u3001Bonavadeur Tokyo \u300c\u9676\u4fa0\u30dc\u30ca\u300d\u306b\u5e30\u5c5e\u3057\u307e\u3059\u3002</p> <p></p>"}]}